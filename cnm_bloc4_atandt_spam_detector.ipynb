{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloc 4 - Analyse prédictive de données non-structurées par l'intelligence artificielle - AT&T Spam Detector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "AT&T Inc. is an American multinational telecommunications holding company, whose history started in 1878 with the foundation of the American District Telegraph Company. It is now the world's third largest telecommunications company by revenue. It is also the third largest provider of mobile phone services in the United States of America.\n",
    "\n",
    "### Problematic\n",
    "\n",
    "AT&T users face a constant exposure to spam messages.\n",
    "\n",
    "The company would like to protect their users by developping an automated spam detector.\n",
    "\n",
    "### Scope\n",
    "\n",
    "To develop a spam detector, AT&T provided a labelled dataset composed of spam and ham messages.\n",
    "\n",
    "### Aim and objectives\n",
    "\n",
    "Overall aim: Predict the spam or ham nature of the message.\n",
    "\n",
    "Objectives:\n",
    "- 1 - Train at least one deep learning model.\n",
    "- 2 - State the achieved performance of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Methods\n",
    "\n",
    "### 1 - Library import\n",
    "\n",
    "### 2 - File reading and basic exploration\n",
    "\n",
    "The dataset was composed of 5572 text messages received by AT&T users and labelled as spam or ham messages. Most messages were contained in a single column, but the content of few of them (a few percent) was split into several columns.\n",
    "\n",
    "Of note, most messages are written in spoken language, and contain many abbreviations.\n",
    "\n",
    "### 3 - Preprocessing \n",
    "\n",
    "First, the few messages that were split were compiled into a single column not to loose information. Then, the data was processed for deep learning, split into train and test sets, and organized in batches.\n",
    "\n",
    "### 4 - Deep learning model training\n",
    "\n",
    "The text of the messages was processed in a very simple manner to avoid discarding words that would not be recognized by pipelines such as spacy. Text was set to lowercase and the punctuation removed to keep most of the content of the messages before vectorization.\n",
    "\n",
    "The model itself is sequential, and only the minimum number of layers was retained. It is composed of only five layers: vectorization, embedding, pooling, 2 x dense. The binary cross entropy was used to quantify the loss and the accuracy was used as a measure of performance. The model was trained for 20 epochs.\n",
    "\n",
    "### 5 - Deep learning model performance\n",
    "\n",
    "The loss and the accuracy reached a plateau after about 15 epochs. At the 15th epoch, the loss reached 0.0266 for the train set and 0.0678 for the validation set. The corresponding accuracies were of 0.9937 for the train set and of 0.9830 for the validation set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Conclusion\n",
    "\n",
    "The dataset of AT&T was rather small (about 5.000 records) but largely sufficient to build a very simple deep learning model with good performances.\n",
    "\n",
    "The model reached more than 98% accuracy on unseen data after only 15 epochs, while staying extremelly simple in its composition (few layers and few neurons).\n",
    "\n",
    "This model is therefore suitable for spam versus ham prediction and thanks to its simplicity, could be easily implemented by the teams of AT&T to protect their users from spam messages."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1 - library import ### ----\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 2 - File reading and basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - import dataset ### ----\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv(\"cnm_bloc4_data.csv\", encoding = \"ISO-8859-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - get basic stats ### ----\n",
    "\n",
    "# print shape of data\n",
    "print(\"Number of rows: {}\".format(data.shape[0]))\n",
    "print(\"Number of columns: {}\".format(data.shape[1]))\n",
    "print()\n",
    "\n",
    "# display dataset\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Dataset display: \")\n",
    "display(data.head())\n",
    "print()\n",
    "\n",
    "# display basic statistics\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = data.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n",
    "\n",
    "# display percentage of missing values in columns and rows\n",
    "percent_nan_col = data.isnull().sum() / data.shape[0] * 100\n",
    "print(\"Percentage of missing values per column:\\n{}\".format(percent_nan_col))\n",
    "print()\n",
    "percent_nan_row = data[data.isnull().all(axis = 1)].shape[0] / data.shape[1] * 100\n",
    "print(\"Percentage of rows fully filled with missing values: {}\".format(percent_nan_row))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 3 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - preprocessing - compile all text columns ### ----\n",
    "\n",
    "# copy data for safety\n",
    "data1 = data.copy()\n",
    "\n",
    "# create column to save full message text\n",
    "data1[\"full_text\"] = data1[\"v2\"]\n",
    "\n",
    "# retreive text from additional columns\n",
    "index_col2 = data.index[data[\"Unnamed: 2\"].notnull()]\n",
    "data1.loc[index_col2,\"full_text\"] += data1.loc[index_col2,\"Unnamed: 2\"]\n",
    "index_col3 = data.index[data[\"Unnamed: 3\"].notnull()]\n",
    "data1.loc[index_col3,\"full_text\"] += data1.loc[index_col3,\"Unnamed: 3\"]\n",
    "index_col4 = data.index[data[\"Unnamed: 4\"].notnull()]\n",
    "data1.loc[index_col4,\"full_text\"] += data1.loc[index_col4,\"Unnamed: 4\"]\n",
    "\n",
    "# drop columns that became useless\n",
    "data1 = data1.drop([\"v2\", \"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - preprocessing - make datasets for deep learning ### ----\n",
    "\n",
    "# encode labels\n",
    "data1[\"v1\"] = data1[\"v1\"].apply(lambda x: 0 if x == \"ham\" else 1)\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data1[\"full_text\"], data1[\"v1\"], test_size = 0.2, \n",
    "    stratify = data1[\"v1\"], random_state = 0)\n",
    "\n",
    "# make tensorflow datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "\n",
    "# organize the datasets in batches\n",
    "train_ds = train_ds.shuffle(len(train_ds)).batch(64)\n",
    "test_ds = test_ds.shuffle(len(test_ds)).batch(64)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 4 - Deep learning model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - deep learning model training - set text preprocessing ### ----\n",
    "\n",
    "# make custom standardization function to remove punctuation\n",
    "def custom_standardization(input_data):\n",
    "  # transform all characters to lowercase\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  # remove punctuation\n",
    "  clean = tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n",
    "  return clean\n",
    "\n",
    "# set vocabulary size and number of words in a sequence\n",
    "vocab_size = 10000\n",
    "sequence_length = 50\n",
    "\n",
    "# initialize vectorization layer to normalize, split, and map strings to integers\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = custom_standardization, \n",
    "    max_tokens = vocab_size, \n",
    "    output_mode = 'int', \n",
    "    output_sequence_length = sequence_length) \n",
    "\n",
    "# Make a text-only dataset and build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x) \n",
    "vectorize_layer.adapt(text_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - deep learning model training - build the model ### ----\n",
    "\n",
    "# set dimension of embedding\n",
    "embedding_dim = 16\n",
    "\n",
    "# build model\n",
    "model = Sequential([\n",
    "  vectorize_layer, \n",
    "  Embedding(vocab_size, embedding_dim, name = \"embedding\"), \n",
    "  GlobalAveragePooling1D(), \n",
    "  Dense(8, activation = 'relu'), \n",
    "  Dense(1, activation = \"sigmoid\") \n",
    "])\n",
    "\n",
    "# set tensorboard to monitor model's training\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = \"logs\")\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer = 'adam',\n",
    "  loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "  metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - deep learning model training - fit the model ### ----\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data = test_ds,\n",
    "    epochs = 20,\n",
    "    callbacks = [tensorboard_callback])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 5 - Deep learning model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 - deep learning model performance - plot performance ### ----\n",
    "\n",
    "# set figure to make subplots\n",
    "fig1 = make_subplots(\n",
    "    rows = 1,\n",
    "    cols = 2,\n",
    "    subplot_titles = (\n",
    "        \"A. Loss\",\n",
    "        \"B. Accuracy\"),\n",
    "    column_widths = [0.40, 0.40],\n",
    "    horizontal_spacing = 0.20)\n",
    "\n",
    "# plot loss for train and validation sets\n",
    "fig1.add_trace(go.Scatter(\n",
    "        name = \"Train\",\n",
    "        x = np.arange(1,len(history.history[\"loss\"])),\n",
    "        y = history.history[\"loss\"],\n",
    "        marker_color = px.colors.qualitative.Vivid[0],\n",
    "        showlegend = True),\n",
    "        row = 1, col = 1)\n",
    "fig1.add_trace(go.Scatter(\n",
    "        name = \"Validation\",\n",
    "        x = np.arange(1,len(history.history[\"val_loss\"])),\n",
    "        y = history.history[\"val_loss\"],\n",
    "        marker_color = px.colors.qualitative.Vivid[1],\n",
    "        showlegend = True),\n",
    "        row = 1, col = 1)\n",
    "\n",
    "# plot accuracy for train and validation sets\n",
    "fig1.add_trace(go.Scatter(\n",
    "        name = \"Train\",\n",
    "        x = np.arange(1,len(history.history[\"accuracy\"])),\n",
    "        y = history.history[\"accuracy\"],\n",
    "        marker_color = px.colors.qualitative.Vivid[0],\n",
    "        showlegend = False),\n",
    "        row = 1, col = 2)\n",
    "fig1.add_trace(go.Scatter(\n",
    "        name = \"Validation\",\n",
    "        x = np.arange(1,len(history.history[\"val_accuracy\"])),\n",
    "        y = history.history[\"val_accuracy\"],\n",
    "        marker_color = px.colors.qualitative.Vivid[1],\n",
    "        showlegend = False),\n",
    "        row = 1, col = 2)\n",
    "\n",
    "# update layout\n",
    "fig1.update_xaxes(title = \"Number of epochs\", tickfont = dict(size = 10), range = [0,21], \n",
    "        tickvals = [0, 5, 10, 15, 20], showgrid = False)\n",
    "fig1.update_yaxes(tickfont = dict(size = 10))\n",
    "fig1.update_layout(\n",
    "        margin = dict(l = 90, t= 120),\n",
    "        title_text = \"Figure 1. Deep learning model performance\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        yaxis = dict(title = \"Loss\", range = [0, 0.75], tickvals = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]),\n",
    "        yaxis2 = dict(title = \"Accuracy\", range = [0.859, 1.01], \n",
    "                tickvals = [0.86, 0.88, 0.90, 0.92, 0.94, 0.96, 0.98, 1.00]),\n",
    "        legend = dict(\n",
    "            orientation = \"h\",\n",
    "            yanchor = \"top\",\n",
    "            y = 1.35,\n",
    "            xanchor = \"left\",\n",
    "            x = 0.35,\n",
    "            font = dict(size = 11)),\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 400)\n",
    "\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 - deep learning model performance - report performance ### ----\n",
    "\n",
    "# report performance at epoch 15\n",
    "print(\"Performance at Epoch 15\")\n",
    "print()\n",
    "print(\"Train loss: {:.4f}\".format(history.history[\"loss\"][14]))\n",
    "print(\"Validation loss: {:.4f}\".format(history.history[\"val_loss\"][14]))\n",
    "print()\n",
    "print(\"Train accuracy: {:.4f}\".format(history.history[\"accuracy\"][14]))\n",
    "print(\"Validation accuracy: {:.4f}\".format(history.history[\"val_accuracy\"][14]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
